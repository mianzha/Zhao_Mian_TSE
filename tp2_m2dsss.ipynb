{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Data Analysis: Communities and Link Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's install the potentially missing libraries \n",
    "(not necessary if you are certain these libs are installed on your system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\progut1\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\progut1\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\progut1\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\progut1\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\progut1\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: networkx in c:\\progut1\\anaconda3\\lib\\site-packages (2.8.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in c:\\progut1\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\progut1\\anaconda3\\lib\\site-packages (from seaborn) (3.7.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\progut1\\anaconda3\\lib\\site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\progut1\\anaconda3\\lib\\site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\progut1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\progut1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\progut1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\progut1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (22.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\progut1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\progut1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\progut1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\progut1\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\progut1\\anaconda3\\lib\\site-packages (from pandas>=0.25->seaborn) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\progut1\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install networkx\n",
    "! pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second, let's import the useful packages\n",
    "You can avoid the first line if you are not using a Jupyter notebook. This line enables the visualization to be displayed in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good, if you have not encounter any problems, we can start working now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First download the dataset (either visit my [professional web page](www.irit.fr/~Yoann.Pitarch) or the Moodle space of this course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously, you must replace the path below by the appropriate path\n",
    "pathData = \"C:/Users/21806538/Downloads/lesmis.gml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Then, navigate through the documentation of the networkx package and find how to load networks in the GML format\n",
    "\n",
    "You can have a look to this file by openning it with a basic text editor. Note that the graph is __undirected__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "g = nx.read_gml(pathData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.4\n"
     ]
    }
   ],
   "source": [
    "print(nx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community detection \n",
    "\n",
    "In this section, we will focus on community detection algorithm. For this, have a look to the networkx package documentation and apply the following community detection algorithms:\n",
    " \n",
    "   1. Kernighan–Lin bipartition algorithm\n",
    "   2. Percolation method\n",
    "   3. Fluid communities algorithm\n",
    "   4. Girvan-Newman method\n",
    " \n",
    "When the number of communities to detect has to be specified as a parameter, you will use the coverage metric to select the appropriate number (ranging from 2 to 5).\n",
    " \n",
    "Finally, for each community algorithm, you will add an attribute to each node of the graph. The value of the attribute will be the identifier of the community tne node belongs to (ranging from 0 to nbCommunity -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Kernighan–Lin bipartition algorithm\n",
    "KL_b = nx.community.kernighan_lin_bisection(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_attribute(partition, algo):\n",
    "    for i in len(partition):\n",
    "        lst = list(algo[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst0 = list(KL_b[0])\n",
    "c0 = {lst0[i]: 0 for i in range(0, len(lst1))}\n",
    "\n",
    "lst1 = list(KL_b[1])\n",
    "c1 = {lst1[i]: 1 for i in range(0, len(lst1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MmePontmercy': 0,\n",
       " 'Napoleon': 0,\n",
       " 'Bahorel': 0,\n",
       " 'Child2': 0,\n",
       " 'Gribier': 0,\n",
       " 'Count': 0,\n",
       " 'Feuilly': 0,\n",
       " 'Cravatte': 0,\n",
       " 'Fauchelevent': 0,\n",
       " 'Gavroche': 0,\n",
       " 'Geborand': 0,\n",
       " 'Jondrette': 0,\n",
       " 'MlleBaptistine': 0,\n",
       " 'Gillenormand': 0,\n",
       " 'Prouvaire': 0,\n",
       " 'Marius': 0,\n",
       " 'Joly': 0,\n",
       " 'LtGillenormand': 0,\n",
       " 'Courfeyrac': 0,\n",
       " 'Enjolras': 0,\n",
       " 'Child1': 0,\n",
       " 'Magnon': 0,\n",
       " 'MlleGillenormand': 0,\n",
       " 'MmeHucheloup': 0,\n",
       " 'Champtercier': 0,\n",
       " 'Pontmercy': 0,\n",
       " 'CountessDeLo': 0,\n",
       " 'MmeMagloire': 0,\n",
       " 'MlleVaubois': 0,\n",
       " 'Myriel': 0,\n",
       " 'Grantaire': 0,\n",
       " 'Bossuet': 0,\n",
       " 'BaronessT': 0,\n",
       " 'OldMan': 0,\n",
       " 'MotherInnocent': 0,\n",
       " 'MmeBurgon': 0,\n",
       " 'Combeferre': 0,\n",
       " 'Mabeuf': 0,\n",
       " 'MotherPlutarch': 0}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Anzelma',\n",
       "  'Babet',\n",
       "  'Bamatabois',\n",
       "  'Blacheville',\n",
       "  'Boulatruelle',\n",
       "  'Brevet',\n",
       "  'Brujon',\n",
       "  'Champmathieu',\n",
       "  'Chenildieu',\n",
       "  'Claquesous',\n",
       "  'Cochepaille',\n",
       "  'Cosette',\n",
       "  'Dahlia',\n",
       "  'Eponine',\n",
       "  'Fameuil',\n",
       "  'Fantine',\n",
       "  'Favourite',\n",
       "  'Gervais',\n",
       "  'Gueulemer',\n",
       "  'Isabeau',\n",
       "  'Javert',\n",
       "  'Judge',\n",
       "  'Labarre',\n",
       "  'Listolier',\n",
       "  'Marguerite',\n",
       "  'MmeDeR',\n",
       "  'MmeThenardier',\n",
       "  'Montparnasse',\n",
       "  'Perpetue',\n",
       "  'Scaufflaire',\n",
       "  'Simplice',\n",
       "  'Thenardier',\n",
       "  'Tholomyes',\n",
       "  'Toussaint',\n",
       "  'Valjean',\n",
       "  'Woman1',\n",
       "  'Woman2',\n",
       "  'Zephine'},\n",
       " {'Bahorel',\n",
       "  'BaronessT',\n",
       "  'Bossuet',\n",
       "  'Champtercier',\n",
       "  'Child1',\n",
       "  'Child2',\n",
       "  'Combeferre',\n",
       "  'Count',\n",
       "  'CountessDeLo',\n",
       "  'Courfeyrac',\n",
       "  'Cravatte',\n",
       "  'Enjolras',\n",
       "  'Fauchelevent',\n",
       "  'Feuilly',\n",
       "  'Gavroche',\n",
       "  'Geborand',\n",
       "  'Gillenormand',\n",
       "  'Grantaire',\n",
       "  'Gribier',\n",
       "  'Joly',\n",
       "  'Jondrette',\n",
       "  'LtGillenormand',\n",
       "  'Mabeuf',\n",
       "  'Magnon',\n",
       "  'Marius',\n",
       "  'MlleBaptistine',\n",
       "  'MlleGillenormand',\n",
       "  'MlleVaubois',\n",
       "  'MmeBurgon',\n",
       "  'MmeHucheloup',\n",
       "  'MmeMagloire',\n",
       "  'MmePontmercy',\n",
       "  'MotherInnocent',\n",
       "  'MotherPlutarch',\n",
       "  'Myriel',\n",
       "  'Napoleon',\n",
       "  'OldMan',\n",
       "  'Pontmercy',\n",
       "  'Prouvaire'})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.set_nodes_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the coverage and performance are (1.0, 0.08680792891319207)\n"
     ]
    },
    {
     "ename": "NetworkXError",
     "evalue": "`partition` is not a valid partition of the nodes of G",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNetworkXError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m):\n\u001b[0;32m      3\u001b[0m     Per \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nx\u001b[38;5;241m.\u001b[39mcommunity\u001b[38;5;241m.\u001b[39mk_clique_communities(g, k \u001b[38;5;241m=\u001b[39m i))  \n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe coverage and performance are\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 35:3\u001b[0m, in \u001b[0;36margmap_partition_quality_32\u001b[1;34m(G, partition)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgUT1\\Anaconda3\\lib\\site-packages\\networkx\\algorithms\\community\\quality.py:56\u001b[0m, in \u001b[0;36m_require_partition\u001b[1;34m(G, partition)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_partition(G, partition):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m G, partition\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`partition` is not a valid partition of the nodes of G\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNetworkXError\u001b[0m: `partition` is not a valid partition of the nodes of G"
     ]
    }
   ],
   "source": [
    "#2. Percolation method\n",
    "for i in range(2,6):\n",
    "    Per = list(nx.community.k_clique_communities(g, k = i))  \n",
    "    print(\"the coverage and performance are\", nx.community.partition_quality(g, Per))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "Per = list(nx.community.k_clique_communities(g, k = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we choose k = 2 for Percolation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the coverage and performance are (0.8543307086614174, 0.5673274094326726)\n",
      "the coverage and performance are (0.8149606299212598, 0.7006151742993848)\n",
      "the coverage and performance are (0.7834645669291339, 0.7792207792207793)\n",
      "the coverage and performance are (0.7677165354330708, 0.8246753246753247)\n"
     ]
    }
   ],
   "source": [
    "#3. Fluid communities algorithm\n",
    "for i in range(2,6):\n",
    "    Flu = list(nx.community.asyn_fluidc(g, k = i)) \n",
    "    print(\"the coverage and performance are\", nx.community.partition_quality(g, Flu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flu = list(nx.community.asyn_fluidc(g, k = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Girvan-Newman method\n",
    "GN = list(nx.community.girvan_newman(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community visualization\n",
    "\n",
    "We will now visualize the result of the communication detection algorithm. For this, we start by filtering out some nodes from the visualisation. Particularly, we would like to filter out nodes that do not belong to any communities according to the percolation method. To do so, you need to create a list that contains the label of nodes belonging to a community according to the percolation method.\n",
    "You can use the following dictionnary to set the visualisation options.\n",
    "\n",
    "```\n",
    "options = {\n",
    "    'node_color' : colorNode, # a list that contains the community id for the nodes we want to plot\n",
    "    'node_size' : 10000, \n",
    "    'cmap' : plt.get_cmap(\"jet\"),\n",
    "    'node_shape' : 'o',\n",
    "    'with_labels' : True, \n",
    "    \"width\" : 0.1, \n",
    "    \"font_size\" : 15,\n",
    "    \"nodelist\" : nodes, # A list that contains the labels of the nodes we want to plot\n",
    "    \"alpha\" : 0.8   \n",
    "}\n",
    "\n",
    "plt.figure(figsize=(18,18))\n",
    "nx.draw(g,**options)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction\n",
    "We now focus on link prediction and tackle this problem using 2 methods: unsupervised and supervised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised\n",
    "We start by the unsupervised perspective. \n",
    "We first build a Panda Series from the edges of the graph and then select a sample of size 50 from this series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, in order to see if metrics we have discussed in this lecture are effective, edges in the sample have to be removed from a copied version of `g`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate some metrics to determine the strength of a potential link between two nodes. We will then select the top 50 potential links and compare them to the one we have just removed to assess how effective are these metrics over this dataset. You will apply the following methodology:\n",
    "\n",
    "  1. Calculate the metrics for all non-existant pairs of nodes\n",
    "  2. Build a dataframe to store these scores and extract the top 50 potential links\n",
    "  3. Use the `isin` function over the sample of edges to count how many removed edges are in the top 50\n",
    "  \n",
    "Repeat this process with the following link prediction metrics :\n",
    "  1. Resource allocation index\n",
    "  2. Jaccard coefficient\n",
    "  3. Adamic-Adar index\n",
    "  4. Preferential attachment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised\n",
    "From previous results, it is hard to say that the above-used features are outstanding... We now try to combine them in a supervised setting. To achieve this, please carrefully apply the following procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/ Set a variable `sizeTestSet` to 50, a variable `sizeTrainingPositiveSet` to the number of edges in g minus the size of the test set and, a variable `sizeTrainingSet` to 2 times the size of the positive training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeTestSet = 50\n",
    "sizeTrainingPositiveSet = len(g.edges()) - sizeTestSet\n",
    "sizeTrainingSet = 2 * sizeTrainingPositiveSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2/ We will build the positive training set and the test set. To do so, first copy the graph `g` into `g_training`. Second, generate a sample of size `sizeTestSet`, denoted by `sampleTest`, from the series of edges of `g_training`. This sample will be your test set (we will apply our model on it and hope the existence of a link will be predicted). Then, remove from `g_training` the edges in `sampleTest`. Finally, convert the remaining edges as a series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/ To balance the training set, we will randomly pick pairs of unconnected vertices (negative class). The number of pairs should be equal to the number of considered connections (positive class) in the training set. Find a way to generate this negative training set and name it `sampleNegativeTraining`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4/ It is now time to calculate the features for each member of the training and test sets. The features list is presented below:\n",
    "  1. size of the shortest path\n",
    "  2. number of shortest paths\n",
    "  3. for each community algorithm, does the vertices associated to a connection belongs to the same community (except -1) : 1 or 0\n",
    "  4. for each link prediction algorithm, the strength of the connection\n",
    "  \n",
    "The feature list is:\n",
    "```\n",
    "features = [\n",
    "    \"lShortestPath\",\n",
    "    \"nbShortestPath\",\n",
    "    \"bipartition\",\n",
    "    \"percolation\",\n",
    "    \"fluid\",\n",
    "    \"girvan\",\n",
    "    \"resource\",\n",
    "    \"jaccard\",\n",
    "    \"adamic\",\n",
    "    \"preferential\",\n",
    "    \"class\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/ Use the following code (and modify it if necessary) to create 2 empty data frames (one for the training set and the other for the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sampleTraining = pd.concat([samplePositiveTraining,sampleNegativeTraining],ignore_index=True)\n",
    "dfTraining = pd.DataFrame(np.zeros((sizeTrainingSet, 11)), columns=features, index= pd.MultiIndex.from_tuples(sampleTraining))\n",
    "dfTest = pd.DataFrame(np.zeros((sizeTestSet, 11)), columns=features, index= pd.MultiIndex.from_tuples(sampleTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/ Write a function `calculateFeatures` with the following specifications:\n",
    "\n",
    "INPUT : \n",
    "  - `sample`: the series of edges you want to calculate the feature values\n",
    "  - `df`: the data frame you want to update\n",
    "  - `training`: True if edges in `sample` are in the training set and False otherwise\n",
    "  - `positive`: True if `training = True` and positive instances are considered\n",
    " \n",
    "OUTPUT\n",
    "  - No output\n",
    "\n",
    "OBJECTIVE\n",
    "\n",
    "Update `df` (the rows such that their indexes are in `sample`) with the feature values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/ Call the function with the apropriate parameters (tips: it should be called 3 times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8/ Apply the following code and conclude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "features = list(dfTraining.columns[:10])\n",
    "y = dfTraining[\"class\"]\n",
    "X = dfTraining[features]\n",
    "dt = DecisionTreeClassifier(min_samples_split=10, random_state=99)\n",
    "dt.fit(X, y)\n",
    "dt.predict(dfTest[features])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
